# agent_Qlearning.py
# Deep Q-Learning Agent for 2048 with Special Tile
import numpy as np
import random
import math
import logic
import constants as c
from collections import deque
import pickle
import os

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: PyTorch not available. DQN agent requires PyTorch.")
    print("Install with: pip install torch")


MOVE_FUNCS = {
    "Up": logic.up,
    "Down": logic.down,
    "Left": logic.left,
    "Right": logic.right,
}

MOVE_NAMES = ["Up", "Down", "Left", "Right"]
NUM_ACTIONS = 4


def _clone(mat):
    return [row[:] for row in mat]


def _apply_special_cell_effect(mat, special_pos):
    """æ”¹ç‰ˆè§„åˆ™ï¼šæˆåŠŸç§»åŠ¨åï¼Œç‰¹æ®Šæ ¼å­ä¸Šçš„å€¼å¦‚æœ >2ï¼Œåˆ™æ•´é™¤2ã€‚"""
    if special_pos is None:
        return mat
    i, j = special_pos
    if mat[i][j] > 2:
        mat[i][j] //= 2
    return mat


def _log2(v):
    """è®¡ç®— log2ï¼Œç©ºå€¼ä¸º 0"""
    return 0 if v <= 0 else int(math.log2(v))


class QNetwork(nn.Module):
    """
    Deep Q-Network: è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼
    """
    def __init__(self, state_size=18, action_size=4, hidden_size=256):
        """
        state_size: çŠ¶æ€ç»´åº¦ (4x4æ£‹ç›˜ + 2ä¸ªç‰¹æ®Šæ ¼åæ ‡ = 16 + 2 = 18)
        action_size: åŠ¨ä½œæ•°é‡ (4ä¸ªæ–¹å‘)
        """
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x


class ReplayBuffer:
    """
    ç»éªŒå›æ”¾ç¼“å†²åŒº
    """
    def __init__(self, capacity=100000):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹ç»éªŒ"""
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            np.array(states),
            np.array(actions),
            np.array(rewards),
            np.array(next_states),
            np.array(dones)
        )
    
    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """
    Deep Q-Learning Agent for 2048 with Special Tile
    """
    
    def __init__(self, 
                 special_pos=None,
                 state_size=18,
                 action_size=4,
                 hidden_size=256,
                 learning_rate=0.001,
                 gamma=0.99,
                 epsilon_start=1.0,
                 epsilon_end=0.01,
                 epsilon_decay=0.995,
                 memory_size=100000,
                 batch_size=64,
                 target_update_freq=1000,
                 device=None,
                 auto_detect_special=True):
        """
        åˆå§‹åŒ– DQN Agent
        
        Args:
            special_pos: ç‰¹æ®Šæ ¼å­ä½ç½® (i, j)
            state_size: çŠ¶æ€ç»´åº¦
            action_size: åŠ¨ä½œæ•°é‡
            hidden_size: éšè—å±‚å¤§å°
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon_start: åˆå§‹æ¢ç´¢ç‡
            epsilon_end: æœ€ç»ˆæ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            memory_size: ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
            batch_size: æ‰¹æ¬¡å¤§å°
            target_update_freq: ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
            device: è®¡ç®—è®¾å¤‡ (cpu/cuda)
        """
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch is required for DQN agent")
        
        self.special_pos = special_pos
        self.auto_detect_special = auto_detect_special
        self.detected_special_pos = None  # æ£€æµ‹åˆ°çš„ç‰¹æ®Šæ ¼ä½ç½®
        self.special_pos_history = []  # ç”¨äºæ£€æµ‹ç‰¹æ®Šæ ¼ä½ç½®çš„å†å²è®°å½•
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        self.update_counter = 0
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        # Q-network å’Œç›®æ ‡ç½‘ç»œ
        self.q_network = QNetwork(state_size, action_size, hidden_size).to(self.device)
        self.target_network = QNetwork(state_size, action_size, hidden_size).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = ReplayBuffer(memory_size)
        
        # å¥–åŠ±å‡½æ•°å‚æ•°
        self.large_tile_threshold = 32  # å¤§æ•°å­—é˜ˆå€¼
        self.small_tile_threshold = 8   # å°æ•°å­—é˜ˆå€¼
        self.large_penalty = -100       # å¤§æ•°å­—è¿›å…¥ç‰¹æ®Šæ ¼æƒ©ç½š
        self.small_reward = 10          # å°æ•°å­—è¿›å…¥ç‰¹æ®Šæ ¼å¥–åŠ±
        self.terminal_penalty = -1000   # æ¸¸æˆç»“æŸæƒ©ç½š
        self.merge_reward_scale = 10    # åˆå¹¶å¥–åŠ±ç¼©æ”¾
    
    def detect_special_position(self, prev_mat, next_mat):
        """
        é€šè¿‡è§‚å¯Ÿç§»åŠ¨å‰åçš„çŸ©é˜µå˜åŒ–æ¥æ£€æµ‹ç‰¹æ®Šæ ¼ä½ç½®
        ç‰¹æ®Šæ ¼çš„ç‰¹å¾ï¼šç§»åŠ¨åï¼Œå¦‚æœæŸä¸ªæ ¼å­çš„å€¼è¢«å‡åŠï¼Œä¸”è¯¥ä½ç½®åœ¨ä¸­å¿ƒåŒºåŸŸï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šæ ¼
        
        Args:
            prev_mat: ç§»åŠ¨å‰çš„çŸ©é˜µ
            next_mat: ç§»åŠ¨åçš„çŸ©é˜µï¼ˆå·²åº”ç”¨ç‰¹æ®Šæ ¼æ•ˆæœï¼‰
            
        Returns:
            detected_pos: æ£€æµ‹åˆ°çš„ç‰¹æ®Šæ ¼ä½ç½® (i, j) æˆ– None
        """
        # å¦‚æœå·²ç»çŸ¥é“ç‰¹æ®Šæ ¼ä½ç½®ï¼Œç›´æ¥è¿”å›
        if self.special_pos is not None:
            return self.special_pos
        
        # å¦‚æœå·²ç»æ£€æµ‹åˆ°ï¼Œç›´æ¥è¿”å›
        if self.detected_special_pos is not None:
            return self.detected_special_pos
        
        # æ£€æµ‹æ–¹æ³•ï¼šè§‚å¯Ÿå“ªäº›æ ¼å­çš„å€¼åœ¨ç§»åŠ¨åè¢«å‡åŠ
        # ç‰¹æ®Šæ ¼ä½ç½®é€šå¸¸åœ¨ä¸­å¿ƒåŒºåŸŸ (1,1) åˆ° (2,2)
        center_start = (4 - 2) // 2  # = 1
        center_end = center_start + 2  # = 3
        
        for i in range(center_start, center_end):
            for j in range(center_start, center_end):
                prev_val = prev_mat[i][j]
                next_val = next_mat[i][j]
                
                # å¦‚æœå€¼è¢«å‡åŠï¼ˆä¸”ä¸æ˜¯0ï¼‰ï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šæ ¼
                if prev_val > 2 and next_val == prev_val // 2:
                    # è®°å½•è¿™ä¸ªå¯èƒ½çš„ä½ç½®
                    self.special_pos_history.append((i, j))
        
        # å¦‚æœå¤šæ¬¡è§‚å¯Ÿåˆ°åŒä¸€ä¸ªä½ç½®è¢«å‡åŠï¼Œç¡®è®¤å®ƒæ˜¯ç‰¹æ®Šæ ¼
        if len(self.special_pos_history) >= 2:
            # ç»Ÿè®¡æœ€å¸¸å‡ºç°çš„ä½ç½®
            from collections import Counter
            pos_counts = Counter(self.special_pos_history)
            most_common = pos_counts.most_common(1)
            if most_common and most_common[0][1] >= 2:
                self.detected_special_pos = most_common[0][0]
                print(f"Detected special tile position: {self.detected_special_pos}")
                return self.detected_special_pos
        
        return None
    
    def update_special_position(self, special_pos):
        """
        æ›´æ–°ç‰¹æ®Šæ ¼ä½ç½®ï¼ˆä»å¤–éƒ¨è·å–ï¼‰
        
        Args:
            special_pos: ç‰¹æ®Šæ ¼ä½ç½® (i, j)
        """
        self.special_pos = special_pos
        self.detected_special_pos = special_pos
    
    def get_special_position(self):
        """
        è·å–å½“å‰ä½¿ç”¨çš„ç‰¹æ®Šæ ¼ä½ç½®
        
        Returns:
            special_pos: ç‰¹æ®Šæ ¼ä½ç½® (i, j) æˆ– None
        """
        if self.special_pos is not None:
            return self.special_pos
        return self.detected_special_pos
    
    def state_to_vector(self, mat):
        """
        å°†æ¸¸æˆçŠ¶æ€è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
        
        Args:
            mat: 4x4 æ¸¸æˆçŸ©é˜µ
            
        Returns:
            state_vector: çŠ¶æ€å‘é‡ (18ç»´)
                - å‰16ç»´: 4x4æ£‹ç›˜çš„å€¼ï¼ˆlog2ç¼–ç ï¼Œç©ºå€¼ä¸º0ï¼‰
                - å2ç»´: ç‰¹æ®Šæ ¼ä½ç½® (i, j)
        """
        state = []
        
        # æ£‹ç›˜å€¼ï¼ˆlog2ç¼–ç ï¼‰
        for i in range(4):
            for j in range(4):
                state.append(_log2(mat[i][j]))
        
        # ç‰¹æ®Šæ ¼ä½ç½®ï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„æˆ–å·²çŸ¥çš„ï¼‰
        special_pos = self.get_special_position()
        if special_pos is not None:
            state.append(special_pos[0] / 4.0)  # å½’ä¸€åŒ–åˆ° [0, 1]
            state.append(special_pos[1] / 4.0)
        else:
            state.append(0.0)
            state.append(0.0)
        
        return np.array(state, dtype=np.float32)
    
    def calculate_reward(self, state, action, next_state_mat, done):
        """
        è®¡ç®—å¥–åŠ±å‡½æ•°
        
        Args:
            state: å½“å‰çŠ¶æ€çŸ©é˜µ
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            next_state_mat: ä¸‹ä¸€ä¸ªçŠ¶æ€çŸ©é˜µ
            done: æ¸¸æˆæ˜¯å¦ç»“æŸ
            
        Returns:
            reward: å¥–åŠ±å€¼
        """
        reward = 0.0
        
        # 1. åˆå¹¶å¥–åŠ±
        merge_score = self._calculate_merge_score(state, next_state_mat)
        reward += merge_score * self.merge_reward_scale
        
        # 2. ç‰¹æ®Šæ ¼æƒ©ç½š/å¥–åŠ±
        special_penalty = self._calculate_special_tile_penalty(state, next_state_mat)
        reward += special_penalty
        
        # 3. æ¸¸æˆç»“æŸæƒ©ç½š
        if done:
            if logic.game_state(next_state_mat) == 'lose':
                reward += self.terminal_penalty
            elif logic.game_state(next_state_mat) == 'win':
                reward += 1000  # èƒœåˆ©å¥–åŠ±
        
        return reward
    
    def _calculate_merge_score(self, state_mat, next_state_mat):
        """è®¡ç®—åˆå¹¶å¾—åˆ†"""
        # è®¡ç®—åˆå¹¶å‰åçš„æ€»å’Œå·®å¼‚
        state_sum = sum(sum(row) for row in state_mat)
        next_sum = sum(sum(row) for row in next_state_mat)
        
        # åˆå¹¶ä¼šå¢åŠ æ€»å’Œï¼ˆå› ä¸ºä¸¤ä¸ªç›¸åŒæ•°å­—åˆå¹¶æˆä¸€ä¸ªç¿»å€çš„æ•°å­—ï¼‰
        # ä½†å®é™…ä¸Šï¼Œåˆå¹¶åæ€»å’Œä¸å˜ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼æ£€æµ‹
        
        # æ›´å‡†ç¡®çš„æ–¹æ³•ï¼šæ£€æµ‹ç©ºæ ¼æ•°é‡å˜åŒ–
        state_empty = sum(1 for i in range(4) for j in range(4) if state_mat[i][j] == 0)
        next_empty = sum(1 for i in range(4) for j in range(4) if next_state_mat[i][j] == 0)
        
        # ç©ºæ ¼å¢åŠ  = åˆå¹¶å‘ç”Ÿ
        merge_count = next_empty - state_empty
        
        return merge_count
    
    def _calculate_special_tile_penalty(self, state_mat, next_state_mat):
        """è®¡ç®—ç‰¹æ®Šæ ¼æƒ©ç½š/å¥–åŠ±"""
        special_pos = self.get_special_position()
        if special_pos is None:
            return 0.0
        
        i, j = special_pos
        penalty = 0.0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰tileè¿›å…¥äº†ç‰¹æ®Šæ ¼
        state_value = state_mat[i][j]
        next_value = next_state_mat[i][j]
        
        # å¦‚æœç‰¹æ®Šæ ¼ä¸Šçš„å€¼å¢åŠ äº†ï¼Œè¯´æ˜æœ‰tileè¿›å…¥äº†
        if next_value > state_value and next_value > 2:
            # æ£€æŸ¥è¿›å…¥å‰çš„å€¼ï¼ˆå¯èƒ½æ˜¯ç§»åŠ¨å‰çš„å€¼ï¼Œæˆ–è€…åˆå¹¶åçš„å€¼ï¼‰
            # ç®€åŒ–å¤„ç†ï¼šå¦‚æœå½“å‰å€¼ >= é˜ˆå€¼ï¼Œç»™äºˆæƒ©ç½š
            if next_value >= self.large_tile_threshold:
                penalty += self.large_penalty
            elif next_value <= self.small_tile_threshold:
                penalty += self.small_reward
        
        return penalty
    
    def select_action(self, state_mat, training=True):
        """
        é€‰æ‹©åŠ¨ä½œï¼ˆepsilon-greedyç­–ç•¥ï¼‰
        
        Args:
            state_mat: å½“å‰çŠ¶æ€çŸ©é˜µ
            training: æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼ï¼ˆå½±å“æ¢ç´¢ç‡ï¼‰
            
        Returns:
            action: åŠ¨ä½œç´¢å¼• (0=Up, 1=Down, 2=Left, 3=Right)
        """
        if training and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return random.randrange(self.action_size)
        
        # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
        try:
            state_vector = self.state_to_vector(state_mat)
            state_tensor = torch.FloatTensor(state_vector).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                action = q_values.argmax().item()
            
            return action
        except Exception as e:
            # å¦‚æœQ-networkå‡ºé”™ï¼Œè¿”å›éšæœºåŠ¨ä½œ
            print(f"Error in select_action: {e}, returning random action")
            return random.randrange(self.action_size)
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self):
        """
        ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­å­¦ä¹ 
        """
        if len(self.memory) < self.batch_size:
            return
        
        # é‡‡æ ·æ‰¹æ¬¡
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.update_counter += 1
        if self.update_counter % self.target_update_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_end:
            self.epsilon *= self.epsilon_decay
        
        return loss.item()
    
    def _is_board_full(self, mat):
        """æ£€æŸ¥æ£‹ç›˜æ˜¯å¦å·²å¡«æ»¡ï¼ˆæ²¡æœ‰ç©ºæ ¼ï¼‰ã€‚"""
        n = len(mat)
        for i in range(n):
            for j in range(n):
                if mat[i][j] == 0:
                    return False
        return True
    
    def _has_merge_potential(self, mat, move_fn):
        """
        æ£€æŸ¥æŸä¸ªç§»åŠ¨æ–¹å‘æ˜¯å¦å¯èƒ½äº§ç”Ÿåˆå¹¶ã€‚
        æ–¹æ³•ï¼šæ£€æŸ¥ç§»åŠ¨åæ˜¯å¦æœ‰ç©ºæ ¼äº§ç”Ÿï¼ˆåˆå¹¶ä¼šäº§ç”Ÿç©ºæ ¼ï¼‰ã€‚
        """
        next_mat, done = move_fn(_clone(mat))
        if not done:
            return False
        
        # å¦‚æœç§»åŠ¨åäº§ç”Ÿäº†ç©ºæ ¼ï¼Œè¯´æ˜æœ‰åˆå¹¶
        n = len(mat)
        original_empty = sum(1 for i in range(n) for j in range(n) if mat[i][j] == 0)
        next_empty = sum(1 for i in range(n) for j in range(n) if next_mat[i][j] == 0)
        
        return next_empty > original_empty
    
    def choose_move(self, mat, prev_mat=None):
        """
        é€‰æ‹©ç§»åŠ¨ï¼ˆç”¨äºæ¸¸æˆæ¥å£ï¼‰
        ä¼˜åŒ–ï¼šå½“æ£‹ç›˜å¡«æ»¡æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘å¯ä»¥äº§ç”Ÿåˆå¹¶çš„æ–¹å‘ï¼Œé¿å…å¡ä½ã€‚
        
        Args:
            mat: å½“å‰æ¸¸æˆçŸ©é˜µ
            prev_mat: ä¸Šä¸€ä¸ªçŠ¶æ€çŸ©é˜µï¼ˆç”¨äºæ£€æµ‹ç‰¹æ®Šæ ¼ä½ç½®ï¼‰
            
        Returns:
            move_name: ç§»åŠ¨æ–¹å‘åç§° ('Up', 'Down', 'Left', 'Right') æˆ– None
        """
        # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨æ£€æµ‹ä¸”è¿˜æ²¡æœ‰æ£€æµ‹åˆ°ç‰¹æ®Šæ ¼ä½ç½®ï¼Œå°è¯•æ£€æµ‹
        if self.auto_detect_special and prev_mat is not None:
            # å°è¯•æ£€æµ‹ç‰¹æ®Šæ ¼ä½ç½®
            self.detect_special_position(prev_mat, mat)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨ç§»åŠ¨
        available_moves = []
        for move_name, move_fn in MOVE_FUNCS.items():
            next_mat, done = move_fn(_clone(mat))
            if done:
                available_moves.append((move_name, move_fn))
        
        if not available_moves:
            return None
        
        # æ£€æŸ¥æ£‹ç›˜æ˜¯å¦å¡«æ»¡
        is_full = self._is_board_full(mat)
        
        # å¦‚æœæ£‹ç›˜å¡«æ»¡ï¼Œä¼˜å…ˆè€ƒè™‘å¯ä»¥äº§ç”Ÿåˆå¹¶çš„æ–¹å‘ï¼ˆé¿å…å¡ä½ï¼‰
        if is_full:
            moves_with_merges = []
            for move_name, move_fn in available_moves:
                if self._has_merge_potential(mat, move_fn):
                    moves_with_merges.append(move_name)
            
            # å¦‚æœæœ‰å¯ä»¥åˆå¹¶çš„æ–¹å‘ï¼Œä¼˜å…ˆé€‰æ‹©è¿™äº›æ–¹å‘
            if moves_with_merges:
                # ä½¿ç”¨Q-networkè¯„ä¼°è¿™äº›å¯åˆå¹¶çš„æ–¹å‘ï¼Œé€‰æ‹©æœ€ä¼˜çš„
                best_move = None
                best_score = -float("inf")
                
                try:
                    for move_name in moves_with_merges:
                        # è·å–ç§»åŠ¨åçš„çŠ¶æ€
                        move_fn = MOVE_FUNCS[move_name]
                        next_mat, _ = move_fn(_clone(mat))
                        next_mat = _apply_special_cell_effect(next_mat, self.get_special_position())
                        
                        # ä½¿ç”¨Q-networkè¯„ä¼°
                        state_vector = self.state_to_vector(next_mat)
                        state_tensor = torch.FloatTensor(state_vector).unsqueeze(0).to(self.device)
                        
                        with torch.no_grad():
                            q_values = self.q_network(state_tensor)
                            # ä½¿ç”¨æœ€å¤§Qå€¼ä½œä¸ºè¯„åˆ†
                            score = q_values.max().item()
                        
                        if score > best_score:
                            best_score = score
                            best_move = move_name
                    
                    if best_move is not None:
                        return best_move
                except Exception as e:
                    # å¦‚æœQ-networkè¯„ä¼°å‡ºé”™ï¼Œç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªå¯åˆå¹¶çš„æ–¹å‘
                    print(f"Error evaluating moves with Q-network: {e}, using first merge move")
                
                # å¦‚æœè¯„ä¼°å¤±è´¥æˆ–å‡ºé”™ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯åˆå¹¶çš„æ–¹å‘
                return moves_with_merges[0]
            # å¦‚æœæ²¡æœ‰å¯åˆå¹¶çš„æ–¹å‘ï¼Œç»§ç»­æ­£å¸¸æµç¨‹ï¼ˆè™½ç„¶ç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
        
        # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨Q-networké€‰æ‹©åŠ¨ä½œ
        try:
            action = self.select_action(mat, training=False)
            action_name = MOVE_NAMES[action]
            
            # å¦‚æœé€‰æ‹©çš„åŠ¨ä½œå¯ç”¨ï¼Œç›´æ¥è¿”å›
            if action_name in [m[0] for m in available_moves]:
                return action_name
        except Exception as e:
            # å¦‚æœQ-networkå‡ºé”™ï¼Œå›é€€åˆ°éšæœºé€‰æ‹©å¯ç”¨åŠ¨ä½œ
            print(f"Q-network error: {e}, falling back to random selection")
        
        # å¦‚æœé€‰æ‹©çš„åŠ¨ä½œä¸å¯ç”¨ï¼Œæˆ–Q-networkå‡ºé”™ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨åŠ¨ä½œ
        return available_moves[0][0]
    
    def save(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'update_counter': self.update_counter,
        }, filepath)
        print(f"Model saved to {filepath}")
    
    def load(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint.get('epsilon', self.epsilon_end)
        self.update_counter = checkpoint.get('update_counter', 0)
        print(f"Model loaded from {filepath}")


def train_dqn_agent(agent, num_episodes=10000, save_freq=1000, save_path="dqn_model.pth"):
    """
    è®­ç»ƒ DQN Agent
    
    Args:
        agent: DQN Agent å®ä¾‹
        num_episodes: è®­ç»ƒè½®æ•°
        save_freq: ä¿å­˜é¢‘ç‡
        save_path: ä¿å­˜è·¯å¾„
    """
    print("Starting DQN training...")
    print(f"Device: {agent.device}")
    print(f"Special tile position: {agent.get_special_position()}")
    print(f"Auto-detect special: {agent.auto_detect_special}")
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        # åˆå§‹åŒ–æ¸¸æˆ
        state_mat = logic.new_game(c.GRID_LEN)
        state_vector = agent.state_to_vector(state_mat)
        
        total_reward = 0
        steps = 0
        done = False
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action_idx = agent.select_action(state_mat, training=True)
            action_name = MOVE_NAMES[action_idx]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            move_fn = MOVE_FUNCS[action_name]
            next_mat, move_done = move_fn(_clone(state_mat))
            
            if not move_done:
                # å¦‚æœæ— æ³•ç§»åŠ¨ï¼Œå°è¯•å…¶ä»–åŠ¨ä½œ
                available_actions = []
                for name, fn in MOVE_FUNCS.items():
                    test_mat, test_done = fn(_clone(state_mat))
                    if test_done:
                        available_actions.append((name, MOVE_NAMES.index(name)))
                
                if available_actions:
                    action_name, action_idx = random.choice(available_actions)
                    move_fn = MOVE_FUNCS[action_name]
                    next_mat, move_done = move_fn(_clone(state_mat))
                else:
                    done = True
                    break
            
            # åº”ç”¨ç‰¹æ®Šæ ¼æ•ˆæœï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„æˆ–å·²çŸ¥çš„ç‰¹æ®Šæ ¼ä½ç½®ï¼‰
            special_pos = agent.get_special_position()
            next_mat = _apply_special_cell_effect(next_mat, special_pos)
            
            # æ£€æµ‹ç‰¹æ®Šæ ¼ä½ç½®ï¼ˆå¦‚æœå¯ç”¨è‡ªåŠ¨æ£€æµ‹ä¸”è¿˜æ²¡æœ‰æ£€æµ‹åˆ°ï¼‰
            if agent.auto_detect_special and special_pos is None:
                agent.detect_special_position(state_mat, next_mat)
                # é‡æ–°è·å–ï¼ˆå¯èƒ½å·²ç»æ£€æµ‹åˆ°ï¼‰
                special_pos = agent.get_special_position()
            
            # éšæœºç”Ÿæˆæ–°tile
            next_mat = logic.add_two(next_mat)
            
            # æ£€æŸ¥æ¸¸æˆçŠ¶æ€
            game_state = logic.game_state(next_mat)
            if game_state != 'not over':
                done = True
            
            # è®¡ç®—å¥–åŠ±
            reward = agent.calculate_reward(state_mat, action_idx, next_mat, done)
            total_reward += reward
            
            # å­˜å‚¨ç»éªŒ
            next_state_vector = agent.state_to_vector(next_mat)
            agent.remember(state_vector, action_idx, reward, next_state_vector, done)
            
            # å­¦ä¹ 
            loss = agent.replay()
            
            # æ›´æ–°çŠ¶æ€
            state_mat = next_mat
            state_vector = next_state_vector
            steps += 1
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_length = np.mean(episode_lengths[-100:])
            print(f"Episode {episode + 1}/{num_episodes} | "
                  f"Avg Reward: {avg_reward:.2f} | "
                  f"Avg Length: {avg_length:.2f} | "
                  f"Epsilon: {agent.epsilon:.3f}")
        
        # ä¿å­˜æ¨¡å‹
        if (episode + 1) % save_freq == 0:
            agent.save(save_path)
            print(f"  ğŸ’¾ Model saved at episode {episode + 1}")
    
    print("Training completed!")
    agent.save(save_path)
    return episode_rewards, episode_lengths


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºagentï¼ˆä¸¤ç§æ–¹å¼ï¼‰
    
    # æ–¹å¼1ï¼šæä¾›ç‰¹æ®Šæ ¼ä½ç½®
    # special_pos = (1, 1)  # ç‰¹æ®Šæ ¼åœ¨ä¸­å¿ƒ
    # agent = DQNAgent(
    #     special_pos=special_pos,
    #     auto_detect_special=False,  # ç¦ç”¨è‡ªåŠ¨æ£€æµ‹
    #     learning_rate=0.001,
    #     gamma=0.99,
    #     epsilon_start=1.0,
    #     epsilon_end=0.01,
    #     epsilon_decay=0.995,
    #     memory_size=100000,
    #     batch_size=64,
    #     target_update_freq=1000
    # )
    
    # æ–¹å¼2ï¼šå¯ç”¨è‡ªåŠ¨æ£€æµ‹ï¼ˆæ¨èï¼‰
    agent = DQNAgent(
        special_pos=None,  # ä¸æä¾›ç‰¹æ®Šæ ¼ä½ç½®
        auto_detect_special=True,  # å¯ç”¨è‡ªåŠ¨æ£€æµ‹
        learning_rate=0.001,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        memory_size=100000,
        batch_size=64,
        target_update_freq=1000
    )
    
    # è®­ç»ƒ
    # train_dqn_agent(agent, num_episodes=10000, save_path="dqn_2048_model.pth")
    
    # æˆ–è€…åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
    # agent.load("dqn_2048_model.pth")
    
    print("DQN Agent initialized. Use train_dqn_agent() to train or load() to load a model.")
    print("Auto-detection enabled: agent will detect special tile position during gameplay.")
